{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35efed82-7bed-4a60-b1e0-4b7e7b669df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Seed 고정 (재현성을 위해)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13240c15-d3f3-4bdc-9561-f5dff8b08884",
   "metadata": {},
   "source": [
    "# RNN Encoder-Decoder with GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc629df1-fe0d-4c8c-b692-bfb45df7f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Update Gate, Reset Gate, Hidden State\n",
    "        self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.W_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "\n",
    "        # Update Gate 계산\n",
    "        z_t = torch.sigmoid(self.W_z(combined))\n",
    "\n",
    "        # Reset Gate 계산\n",
    "        r_t = torch.sigmoid(self.W_r(combined))\n",
    "\n",
    "        # reset된 h_prev와 input을 이용해 h_tilde 계산\n",
    "        combined_reset = torch.cat([x, r_t * h_prev], dim=1)\n",
    "        h_tilde = torch.tanh(self.W_h(combined_reset))\n",
    "\n",
    "        # 최종 hidden state 계산\n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664c0b27-9db7-4de2-8f9c-3500c7225302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = GRUCell(emb_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, src_len)\n",
    "        embedded = self.dropout(self.embedding(src))  # (batch_size, src_len, emb_dim)\n",
    "        batch_size, src_len, _ = embedded.size()\n",
    "        hidden = torch.zeros(batch_size, self.gru.hidden_size).to(src.device)\n",
    "        \n",
    "        for t in range(src_len):\n",
    "            hidden = self.gru(embedded[:, t, :], hidden)\n",
    "        \n",
    "        return hidden  # (batch_size, hidden_dim)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = GRUCell(emb_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # input: (batch_size)\n",
    "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.dropout(self.embedding(input))  # (batch_size, 1, emb_dim)\n",
    "        embedded = embedded.squeeze(1)  # (batch_size, emb_dim)\n",
    "        \n",
    "        hidden = self.gru(embedded, hidden)  # (batch_size, hidden_dim)\n",
    "        output = self.fc_out(hidden)  # (batch_size, output_dim)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee990b7-98f2-4893-83df-9ed7b9f8dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        # src: (batch_size, src_len)\n",
    "        # trg: (batch_size, trg_len)\n",
    "        \n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n",
    "        \n",
    "        hidden = self.encoder(src)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # 첫 번째 디코더 입력은 <sos> 토큰\n",
    "        input = trg[:, 0]  # (batch_size)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)  # output: (batch_size, output_dim)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # (batch_size)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ff237c-7f0e-45ce-8d76-4de67c266f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 불러오기\n",
    "file_path = 'english_german_dataset.csv'\n",
    "dataset_df = pd.read_csv(file_path)\n",
    "dataset = list(zip(dataset_df['English'], dataset_df['German']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca154e50-5d99-4210-b3e7-52a445fba2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 481\n",
      "Vocabulary: {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'Hello,': 3, 'how': 4, 'are': 5, 'you?': 6, 'Hallo,': 7, 'wie': 8, 'geht': 9, 'es': 10, 'dir?': 11, 'Good': 12, 'morning!': 13, 'Guten': 14, 'Morgen!': 15, 'I': 16, 'love': 17, 'machine': 18, 'learning.': 19, 'Ich': 20, 'liebe': 21, 'maschinelles': 22, 'Lernen.': 23, 'What': 24, 'is': 25, 'your': 26, 'name?': 27, 'Wie': 28, 'heißt': 29, 'du?': 30, 'Nice': 31, 'to': 32, 'meet': 33, 'you.': 34, 'Freut': 35, 'mich,': 36, 'dich': 37, 'kennenzulernen.': 38, 'Thank': 39, 'you': 40, 'very': 41, 'much.': 42, 'Vielen': 43, 'Dank.': 44, 'See': 45, 'later.': 46, 'Bis': 47, 'später.': 48, 'am': 49, 'learning': 50, 'code.': 51, 'lerne': 52, 'zu': 53, 'programmieren.': 54, 'This': 55, 'a': 56, 'pen.': 57, 'Das': 58, 'ist': 59, 'ein': 60, 'Stift.': 61, 'How': 62, 'old': 63, 'alt': 64, 'bist': 65, 'Where': 66, 'the': 67, 'nearest': 68, 'restaurant?': 69, 'Wo': 70, 'das': 71, 'nächste': 72, 'Restaurant?': 73, 'Can': 74, 'help': 75, 'me?': 76, 'Kannst': 77, 'du': 78, 'mir': 79, 'helfen?': 80, 'would': 81, 'like': 82, 'cup': 83, 'of': 84, 'coffee.': 85, 'hätte': 86, 'gerne': 87, 'eine': 88, 'Tasse': 89, 'Kaffee.': 90, 'time': 91, 'it?': 92, 'spät': 93, 'es?': 94, 'from': 95, 'United': 96, 'States.': 97, 'komme': 98, 'aus': 99, 'den': 100, 'Vereinigten': 101, 'Staaten.': 102, 'Do': 103, 'speak': 104, 'English?': 105, 'Sprichst': 106, 'Englisch?': 107, 'need': 108, 'some': 109, 'water.': 110, 'brauche': 111, 'etwas': 112, 'Wasser.': 113, 'Excuse': 114, 'me,': 115, 'where': 116, 'bathroom?': 117, 'Entschuldigung,': 118, 'wo': 119, 'die': 120, 'Toilette?': 121, 'Happy': 122, 'birthday!': 123, 'Alles': 124, 'Gute': 125, 'zum': 126, 'Geburtstag!': 127, 'night.': 128, 'Nacht.': 129, 'doing?': 130, 'Was': 131, 'machst': 132, 'gerade?': 133, 'have': 134, 'pet': 135, 'dog.': 136, 'habe': 137, 'einen': 138, 'Hund': 139, 'als': 140, 'Haustier.': 141, 'Please': 142, 'call': 143, 'me': 144, 'Bitte': 145, 'ruf': 146, 'mich': 147, 'später': 148, 'an.': 149, 'The': 150, 'weather': 151, 'nice': 152, 'today.': 153, 'Wetter': 154, 'heute': 155, 'schön.': 156, \"Let's\": 157, 'go': 158, 'for': 159, 'walk.': 160, 'Lass': 161, 'uns': 162, 'spazieren': 163, 'gehen.': 164, 'cooking': 165, 'dinner.': 166, 'koche': 167, 'Abendessen.': 168, 'My': 169, 'favorite': 170, 'color': 171, 'blue.': 172, 'Meine': 173, 'Lieblingsfarbe': 174, 'blau.': 175, 'was': 176, 'day?': 177, 'war': 178, 'dein': 179, 'Tag?': 180, 'Are': 181, 'coming': 182, 'with': 183, 'us?': 184, 'Kommst': 185, 'mit': 186, 'uns?': 187, 'buy': 188, 'groceries.': 189, 'muss': 190, 'Lebensmittel': 191, 'kaufen.': 192, 'book': 193, 'interesting.': 194, 'Dieses': 195, 'Buch': 196, 'interessant.': 197, 'She': 198, 'likes': 199, 'read': 200, 'books.': 201, 'Sie': 202, 'liest': 203, 'Bücher.': 204, 'He': 205, 'good': 206, 'friend.': 207, 'Er': 208, 'guter': 209, 'Freund.': 210, 'play': 211, 'piano?': 212, 'Klavier': 213, 'spielen?': 214, 'phone': 215, 'charger?': 216, 'Hast': 217, 'Ladegerät?': 218, 'want': 219, 'watch': 220, 'movie.': 221, 'möchte': 222, 'Film': 223, 'ansehen.': 224, 'going': 225, 'library.': 226, 'in': 227, 'Bibliothek.': 228, 'works': 229, 'an': 230, 'office.': 231, 'arbeitet': 232, 'einem': 233, 'Büro.': 234, 'turn': 235, 'on': 236, 'light.': 237, 'mach': 238, 'Licht': 239, 'It': 240, 'raining': 241, 'outside.': 242, 'Es': 243, 'regnet': 244, 'draußen.': 245, 'food?': 246, 'Lieblingsessen?': 247, 'coffee?': 248, 'Magst': 249, 'Kaffee?': 250, 'looking': 251, 'job.': 252, 'suche': 253, 'Job.': 254, 'train': 255, 'delayed.': 256, 'Der': 257, 'Zug': 258, 'hat': 259, 'Verspätung.': 260, 'my': 261, 'mom.': 262, 'meine': 263, 'Mutter': 264, 'anrufen.': 265, 'closest': 266, 'bank?': 267, 'Bank?': 268, 'forgot': 269, 'password.': 270, 'mein': 271, 'Passwort': 272, 'vergessen.': 273, 'bill,': 274, 'please?': 275, 'Kann': 276, 'ich': 277, 'bitte': 278, 'Rechnung': 279, 'haben?': 280, 'pizza': 281, 'delicious.': 282, 'Diese': 283, 'Pizza': 284, 'lecker.': 285, 'car': 286, 'needs': 287, 'gas.': 288, 'Auto': 289, 'braucht': 290, 'Benzin.': 291, 'any': 292, 'help?': 293, 'Brauchst': 294, 'Hilfe?': 295, 'reading': 296, 'book.': 297, 'lese': 298, 'Buch.': 299, 'has': 300, 'beautiful': 301, 'smile.': 302, 'schönes': 303, 'Lächeln.': 304, 'teacher.': 305, 'Lehrer.': 306, 'close': 307, 'door.': 308, 'schließe': 309, 'Tür.': 310, 'flight': 311, 'Flug': 312, 'hatte': 313, 'borrow': 314, 'pen?': 315, 'Stift': 316, 'leihen?': 317, 'listening': 318, 'music.': 319, 'höre': 320, 'Musik.': 321, 'wearing': 322, 'red': 323, 'dress.': 324, 'trägt': 325, 'rotes': 326, 'Kleid.': 327, 'soccer.': 328, 'spielt': 329, 'Fußball.': 330, \"don't\": 331, 'understand': 332, 'this.': 333, 'verstehe': 334, 'nicht.': 335, 'hungry?': 336, 'Hunger?': 337, 'movie': 338, 'amazing.': 339, 'großartig.': 340, 'feeling': 341, 'tired.': 342, 'fühle': 343, 'müde.': 344, 'kind.': 345, 'sehr': 346, 'nett.': 347, 'not': 348, 'here': 349, 'yet.': 350, 'noch': 351, 'nicht': 352, 'hier.': 353, 'did': 354, 'go?': 355, 'gewesen?': 356, 'studying?': 357, 'studierst': 358, 'will': 359, 'be': 360, 'back': 361, 'soon.': 362, 'bald': 363, 'zurück.': 364, 'hear': 365, 'hören?': 366, 'student.': 367, 'bin': 368, 'Student.': 369, 'wait': 370, 'here.': 371, 'warte': 372, 'cake': 373, 'sweet.': 374, 'Dieser': 375, 'Kuchen': 376, 'süß.': 377, 'know': 378, 'answer?': 379, 'Kennst': 380, 'Antwort?': 381, 'feel': 382, 'well.': 383, 'Mir': 384, 'gut.': 385, 'do': 386, 'live?': 387, 'wohnst': 388, 'show': 389, 'starts': 390, 'at': 391, '8': 392, 'PM.': 393, 'Die': 394, 'Show': 395, 'beginnt': 396, 'um': 397, '20': 398, 'Uhr.': 399, 'like?': 400, 'Wetter?': 401, 'new': 402, 'phone.': 403, 'neues': 404, 'Telefon.': 405, 'coffee': 406, 'too': 407, 'hot.': 408, 'Kaffee': 409, 'heiß.': 410, 'writing': 411, 'email.': 412, 'schreibe': 413, 'E-Mail.': 414, 'best': 415, 'beste': 416, 'Freundin.': 417, 'car.': 418, 'Auto.': 419, 'lunch?': 420, 'Wann': 421, 'gibt': 422, 'Mittagessen?': 423, 'travel.': 424, 'reisen.': 425, 'place': 426, 'beautiful.': 427, 'Ort': 428, 'language.': 429, 'neue': 430, 'Sprache.': 431, 'hilf': 432, 'dabei.': 433, 'cat.': 434, 'Katze.': 435, 'tall.': 436, 'groß.': 437, 'keys.': 438, 'Schlüssel': 439, 'siblings?': 440, 'Geschwister?': 441, 'first': 442, 'erstes': 443, 'Mal': 444, 'sunset': 445, 'Sonnenuntergang': 446, 'home': 447, 'now.': 448, 'gehe': 449, 'jetzt': 450, 'nach': 451, 'Hause.': 452, 'take': 453, 'seat.': 454, 'nimm': 455, 'Platz.': 456, 'pass': 457, 'salt?': 458, 'Salz': 459, 'reichen?': 460, 'working': 461, 'project.': 462, 'arbeite': 463, 'Projekt.': 464, 'beach': 465, 'crowded.': 466, 'Strand': 467, 'überfüllt.': 468, 'tomorrow.': 469, 'rufe': 470, 'morgen': 471, 'bag?': 472, 'Tasche?': 473, 'food': 474, 'spicy.': 475, 'Essen': 476, 'scharf.': 477, 'waiting': 478, 'auf': 479, 'dich.': 480}\n"
     ]
    }
   ],
   "source": [
    "# 2. 단어 사전 구축\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "def build_vocab(pairs, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for src, trg in pairs:\n",
    "        for word in src.split():\n",
    "            counter[word] += 1\n",
    "        for word in trg.split():\n",
    "            counter[word] += 1\n",
    "    vocab = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2}\n",
    "    index = 3\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(dataset, min_freq=1)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04cb5c9-8e9e-4c1a-92a7-ecf291237708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Tensor shape: torch.Size([103, 9])\n",
      "Target Tensor shape: torch.Size([103, 8])\n"
     ]
    }
   ],
   "source": [
    "# 3. 문장을 인덱스로 변환하는 함수 정의\n",
    "def tokenize(sentence, vocab):\n",
    "    return [vocab.get(word, vocab[PAD_TOKEN]) for word in sentence.split()]\n",
    "\n",
    "def add_special_tokens(token_ids, sos_token, eos_token, max_len=None):\n",
    "    token_ids = [sos_token] + token_ids + [eos_token]\n",
    "    if max_len:\n",
    "        token_ids += [sos_token] * (max_len - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "# 4. 데이터 전처리\n",
    "src_sentences = [pair[0] for pair in dataset]\n",
    "trg_sentences = [pair[1] for pair in dataset]\n",
    "\n",
    "src_indices = [tokenize(sentence, vocab) for sentence in src_sentences]\n",
    "trg_indices = [tokenize(sentence, vocab) for sentence in trg_sentences]\n",
    "\n",
    "max_src_len = max(len(seq) for seq in src_indices) + 2\n",
    "max_trg_len = max(len(seq) for seq in trg_indices) + 2\n",
    "\n",
    "src_indices = [add_special_tokens(seq, vocab[SOS_TOKEN], vocab[EOS_TOKEN], max_len=max_src_len) for seq in src_indices]\n",
    "trg_indices = [add_special_tokens(seq, vocab[SOS_TOKEN], vocab[EOS_TOKEN], max_len=max_trg_len) for seq in trg_indices]\n",
    "\n",
    "src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "trg_tensor = torch.tensor(trg_indices, dtype=torch.long)\n",
    "\n",
    "print(\"Source Tensor shape:\", src_tensor.shape)\n",
    "print(\"Target Tensor shape:\", trg_tensor.shape)\n",
    "\n",
    "# 5. Train/Validation 분할 (80:20)\n",
    "train_src, val_src, train_trg, val_trg = train_test_split(\n",
    "    src_tensor, trg_tensor, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f71609-db3a-47da-a309-f0ef863723f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training set size: 82\n",
      "Validation set size: 21\n"
     ]
    }
   ],
   "source": [
    "# 6. 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "train_src, train_trg = train_src.to(device), train_trg.to(device)\n",
    "val_src, val_trg = val_src.to(device), val_trg.to(device)\n",
    "\n",
    "print(f'Training set size: {train_src.size(0)}')\n",
    "print(f'Validation set size: {val_src.size(0)}')\n",
    "\n",
    "# 7. 하이퍼파라미터 설정\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# 8. 모델 초기화\n",
    "encoder = Encoder(input_dim=INPUT_DIM, emb_dim=ENC_EMB_DIM, hidden_dim=HIDDEN_DIM, dropout=ENC_DROPOUT)\n",
    "decoder = Decoder(output_dim=OUTPUT_DIM, emb_dim=DEC_EMB_DIM, hidden_dim=HIDDEN_DIM, dropout=DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=TEACHER_FORCING_RATIO).to(device)\n",
    "\n",
    "# 9. 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 10. 학습 및 검증 함수 정의\n",
    "def train_epoch(model, optimizer, criterion, src, trg):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, trg)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output[:, 1:].reshape(-1, output_dim)\n",
    "    trg = trg[:, 1:].reshape(-1)\n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_epoch(model, criterion, src, trg):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3c46a03-7565-4823-a703-6aba9a97acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 학습 시작 ===\n",
      "Epoch [1/1000], Train Loss: 6.2047, Val Loss: 6.1595\n",
      "Epoch [100/1000], Train Loss: 2.5352, Val Loss: 4.8587\n",
      "Epoch [200/1000], Train Loss: 1.7110, Val Loss: 5.0622\n",
      "Epoch [300/1000], Train Loss: 0.6194, Val Loss: 5.7587\n",
      "Epoch [400/1000], Train Loss: 0.2437, Val Loss: 5.6741\n",
      "Epoch [500/1000], Train Loss: 0.0794, Val Loss: 6.0228\n",
      "Epoch [600/1000], Train Loss: 0.0367, Val Loss: 6.6820\n",
      "Epoch [700/1000], Train Loss: 0.0206, Val Loss: 6.5860\n",
      "Epoch [800/1000], Train Loss: 0.0144, Val Loss: 6.9552\n",
      "Epoch [900/1000], Train Loss: 0.0101, Val Loss: 6.6054\n",
      "Epoch [1000/1000], Train Loss: 0.0076, Val Loss: 7.0580\n"
     ]
    }
   ],
   "source": [
    "# 11. 학습 루프\n",
    "print(\"=== 학습 시작 ===\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_src, train_trg)\n",
    "    val_loss = evaluate_epoch(model, criterion, val_src, val_trg)\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f'Epoch [{epoch}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58bea4d6-633b-4071-b018-58235effdac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 예측을 위한 디코딩 함수 정의\n",
    "def decode(indices, inv_vocab):\n",
    "    sentences = []\n",
    "    for seq in indices:\n",
    "        words = []\n",
    "        for idx in seq:\n",
    "            word = inv_vocab.get(idx.item(), PAD_TOKEN)\n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "            if word not in [SOS_TOKEN, PAD_TOKEN]:\n",
    "                words.append(word)\n",
    "        sentences.append(' '.join(words))\n",
    "    return sentences\n",
    "\n",
    "# 13. 검증 데이터에 대한 예측 수행\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(val_src, val_trg)\n",
    "    pred_indices = output.argmax(dim=2)\n",
    "\n",
    "pred_sentences = decode(pred_indices, inv_vocab)\n",
    "val_sentences = decode(val_trg, inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38238ac-9922-45f3-b685-423ce9b753dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 검증 결과 ===\n",
      "Source: This book is interesting.\n",
      "Target: Dieses Buch ist interessant.\n",
      "Predicted: Das ist mein erstes Mal\n",
      "---\n",
      "Source: What are you studying?\n",
      "Target: Was studierst du?\n",
      "Predicted: Was machst du gewesen?\n",
      "---\n",
      "Source: The movie was amazing.\n",
      "Target: Der Film war großartig.\n",
      "Predicted: Der Flug hatte Verspätung.\n",
      "---\n",
      "Source: Can I have the bill, please?\n",
      "Target: Kann ich bitte die Rechnung haben?\n",
      "Predicted: Kann ich mir einen Stift\n",
      "---\n",
      "Source: I am looking for a job.\n",
      "Target: Ich suche einen Job.\n",
      "Predicted: Ich gehe jetzt nach Hause.\n",
      "---\n",
      "Source: What is your favorite food?\n",
      "Target: Was ist dein Lieblingsessen?\n",
      "Predicted: Wie machst du gewesen?\n",
      "---\n",
      "Source: I forgot my keys.\n",
      "Target: Ich habe meine Schlüssel vergessen.\n",
      "Predicted: Ich habe mein Passwort vergessen.\n",
      "---\n",
      "Source: Where is the closest bank?\n",
      "Target: Wo ist die nächste Bank?\n",
      "Predicted: Wann bist ist lecker.\n",
      "---\n",
      "Source: Where is the nearest restaurant?\n",
      "Target: Wo ist das nächste Restaurant?\n",
      "Predicted: Er bist du gewesen?\n",
      "---\n",
      "Source: Hello, how are you?\n",
      "Target: Hallo, wie geht es dir?\n",
      "Predicted: Der Flug hatte Verspätung.\n",
      "---\n",
      "Source: Happy birthday!\n",
      "Target: Alles Gute zum Geburtstag!\n",
      "Predicted: Bitte du Hunger?\n",
      "---\n",
      "Source: She likes to read books.\n",
      "Target: Sie liest gerne Bücher.\n",
      "Predicted: Sie hat ein schönes Lächeln.\n",
      "---\n",
      "Source: I am working on a project.\n",
      "Target: Ich arbeite an einem Projekt.\n",
      "Predicted: Ich komme bald zurück.\n",
      "---\n",
      "Source: This place is beautiful.\n",
      "Target: Dieser Ort ist schön.\n",
      "Predicted: Diese Kuchen ist süß.\n",
      "---\n",
      "Source: The show starts at 8 PM.\n",
      "Target: Die Show beginnt um 20 Uhr.\n",
      "Predicted: Das Auto braucht Benzin.\n",
      "---\n",
      "Source: Nice to meet you.\n",
      "Target: Freut mich, dich kennenzulernen.\n",
      "Predicted: Kommst du die Antwort?\n",
      "---\n",
      "Source: The coffee is too hot.\n",
      "Target: Der Kaffee ist zu heiß.\n",
      "Predicted: Der Strand ist überfüllt.\n",
      "---\n",
      "Source: Can you play the piano?\n",
      "Target: Kannst du Klavier spielen?\n",
      "Predicted: Kannst du mir das Salz\n",
      "---\n",
      "Source: I would like a cup of coffee.\n",
      "Target: Ich hätte gerne eine Tasse Kaffee.\n",
      "Predicted: Ich lerne eine neue Sprache.\n",
      "---\n",
      "Source: My favorite color is blue.\n",
      "Target: Meine Lieblingsfarbe ist blau.\n",
      "Predicted: Ich Wetter ist heute schön.\n",
      "---\n",
      "Source: Where is my bag?\n",
      "Target: Wo ist meine Tasche?\n",
      "Predicted: Sie bist du?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 14. 결과 출력\n",
    "print(\"=== 검증 결과 ===\")\n",
    "for i in range(len(val_src)):\n",
    "    src_sentence = decode(val_src[i].unsqueeze(0), inv_vocab)[0]\n",
    "    trg_sentence = val_sentences[i]\n",
    "    pred_sentence = pred_sentences[i]\n",
    "    print(f\"Source: {src_sentence}\")\n",
    "    print(f\"Target: {trg_sentence}\")\n",
    "    print(f\"Predicted: {pred_sentence}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341151c-5fc0-4414-8d6c-4a03c0ffda5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
